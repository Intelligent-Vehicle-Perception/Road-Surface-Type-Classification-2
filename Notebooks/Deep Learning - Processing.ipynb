{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3"},"colab":{"name":"Deep Learning - Processing.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"m3WpWEwwlV5K","colab_type":"text"},"source":["<h1>Deep Learning - Processing</h1>"]},{"cell_type":"markdown","metadata":{"id":"nR00SdqulV5S","colab_type":"text"},"source":["<h5>Data Parameters</h5>"]},{"cell_type":"code","metadata":{"id":"y6Jrok4nlV5T","colab_type":"code","colab":{}},"source":["try:\n","    from google.colab import output\n","    IN_COLAB = True\n","except:\n","    output = None\n","    IN_COLAB = False\n","\n","if IN_COLAB:\n","    datasets_folder = '/drive/My Drive/Colab Notebooks/DataSets/'\n","    experiments_folder = '/drive/My Drive/Colab Notebooks/Experiments/'\n","else:\n","    datasets_folder = '/Google Drive/Colab Notebooks/DataSets/'\n","    experiments_folder = '/Google Drive/Colab Notebooks/Experiments/'\n","\n","print(\"In Colab:\", IN_COLAB)\n","print(\"Dataset Folder:\", datasets_folder)\n","print(\"Experiments Folder:\", experiments_folder)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qBLe_2hXlV5M","colab_type":"text"},"source":["<h5>Importing Packages</h5>"]},{"cell_type":"code","metadata":{"id":"hEH4jRqFsEcp","colab_type":"code","colab":{}},"source":["try:\n","    import livelossplot\n","except:\n","    !pip install livelossplot --quiet"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3FkhkhXHlV5O","colab_type":"code","colab":{}},"source":["import os\n","import gc\n","import shutil\n","import time\n","import numpy as np\n","import pandas as pd\n","import librosa as lr\n","import librosa.display\n","import seaborn as sns\n","import scipy.signal as signal\n","from matplotlib import colors\n","import matplotlib.pyplot as plt\n","from IPython.display import display\n","from tqdm.notebook import tqdm\n","from sklearn.preprocessing import MinMaxScaler, RobustScaler\n","from sklearn.metrics import confusion_matrix"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q7kPsJqpX-GD","colab_type":"code","colab":{}},"source":["# -------------------------------------\n","# TensorFlow/Keras Selection\n","# -------------------------------------\n","\n","# useKerasTfV1=False\n","# useKerasTfV2=False\n","# useTfV2=True\n","\n","if useKerasTfV1:\n","\n","    if IN_COLAB:\n","        %tensorflow_version 1.x\n","    \n","    import tensorflow as tf  \n","    import keras\n","    from keras.models import Sequential\n","    from keras.callbacks import History, ModelCheckpoint, CSVLogger, EarlyStopping\n","    from keras.layers import Activation, Dense, Dropout, SpatialDropout1D, Conv1D, TimeDistributed, MaxPooling1D, Flatten, ConvLSTM2D, Bidirectional, BatchNormalization, GlobalAvgPool1D, GlobalAveragePooling1D, MaxPooling1D, CuDNNLSTM as LSTM\n","    from keras.utils import plot_model\n","    \n","    from livelossplot import PlotLossesKeras\n","\n","elif useKerasTfV2:\n","\n","    import keras\n","    from keras.models import Sequential\n","    from keras.callbacks import History, ModelCheckpoint, CSVLogger, EarlyStopping\n","    from keras.layers import Activation, Dense, Dropout, SpatialDropout1D, Conv1D, TimeDistributed, MaxPooling1D, Flatten, ConvLSTM2D, Bidirectional, BatchNormalization, GlobalAvgPool1D, GlobalAveragePooling1D, MaxPooling1D, LSTM\n","    from keras.utils import plot_model\n","    \n","    from livelossplot import PlotLossesKerasTF as PlotLossesKeras\n","    \n","    if not(keras.__version__.startswith(\"2.3\")):\n","        !pip install \"keras>=2.3.0\"\n","\n","elif useTfV2:\n","    \n","    import tensorflow as tf\n","    from tensorflow.keras.models import Sequential\n","    from tensorflow.keras.callbacks import History, ModelCheckpoint, CSVLogger, EarlyStopping\n","    from tensorflow.keras.layers import Activation, Dense, Dropout, SpatialDropout1D, Conv1D, TimeDistributed, MaxPooling1D, Flatten, ConvLSTM2D, Bidirectional, BatchNormalization, GlobalAvgPool1D, GlobalAveragePooling1D, MaxPooling1D, LSTM, GRU\n","    from tensorflow.keras.utils import plot_model\n","    \n","    from livelossplot import PlotLossesKerasTF as PlotLossesKeras\n","\n","try:\n","    print(\"Using Tensorflow\", tf.__version__)\n","except:\n","    pass\n","\n","try:\n","    print(\"Using Keras\", keras.__version__)\n","except:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JB_YhW184c2I","colab_type":"code","colab":{}},"source":["def showDevices():\n","    print(\"Devices Tensorflow\")\n","\n","    if useKerasTfV1:\n","        print(tf.compat.v1.config.list_physical_devices())\n","    else:\n","        print(tf.config.list_physical_devices())\n","\n","    print(\"\")\n","    print(\"Devices Nvidia\")\n","    !nvidia-smi"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LYpqWHmElV5X","colab_type":"text"},"source":["<h5>Data Functions</h5>"]},{"cell_type":"code","metadata":{"id":"wb0kX7qDvRO6","colab_type":"code","colab":{}},"source":["# Frequency Domain Params Default\n","fft_window = 100\n","fft_step = 1\n","fft_mode_label=True"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eMcwk8WrlV5Y","colab_type":"code","colab":{}},"source":["def getDataSets(folder=datasets_folder):\n","    \n","    \"\"\"Load raw datasets from the disk.\n","\n","    Args:\n","        folder (str): Root folder of PVS datasets. Within this root folder are the PVS folders and their files.\n","\n","    Returns:\n","        dict: datasets in a dict form: \n","        { \n","            \"pvs_x\": { \n","                \"left\": DataFrame, \n","                \"right\": DataFrame, \n","                \"labels\": DataFrame \n","            } \n","        }\n","    \"\"\"\n","\n","    datasets = {}\n","    \n","    for i in range(1, 10):\n","        \n","        dataset_folder = os.path.join(folder, \"PVS \" + str(i))\n","        \n","        # left =   pd.read_csv(os.path.join(dataset_folder, 'dataset_gps_mpu_left.csv'),  float_precision=\"high\")\n","        # right =  pd.read_csv(os.path.join(dataset_folder, 'dataset_gps_mpu_right.csv'), float_precision=\"high\")\n","        # labels = pd.read_csv(os.path.join(dataset_folder, 'dataset_labels.csv'),        float_precision=\"high\")\n","        left =   pd.read_csv(os.path.join(dataset_folder, 'dataset_gps_mpu_left.csv'), dtype=np.float32)\n","        right =  pd.read_csv(os.path.join(dataset_folder, 'dataset_gps_mpu_right.csv'), dtype=np.float32)\n","        labels = pd.read_csv(os.path.join(dataset_folder, 'dataset_labels.csv'), dtype=np.uint8)\n","        \n","        datasets[\"pvs_\" + str(i)] = {\n","            \"left\": left,\n","            \"right\": right,\n","            \"labels\": labels\n","        }\n","    \n","    return datasets\n","\n","def getFields(acc=False, gyro=False, mag=False, temp=False, speed=False, location=False, below_suspension=False, above_suspension=False, dashboard=False):\n","    \n","    \"\"\"Get fields names filtering by data type and placement.\n","\n","    Args:\n","        acc (bool): to return accelerometer fields.\n","            (default is False)\n","        gyro (bool): to return gyroscope fields.\n","            (default is False)\n","        mag (bool): to return magnetometer fields.\n","            (default is False)\n","        temp (bool): to return temperature field.\n","            (default is False)\n","        speed (bool): to return speed field.\n","            (default is False)\n","        location (bool): to return GPS location fields.\n","            (default is False)\n","        below_suspension (bool): to return fields of data sampled next and below suspension.\n","            (default is False)\n","        above_suspension (bool): to return fields of data sampled next and above suspension.\n","            (default is False)\n","        dashboard (bool): to return fields of data sampled in the dashboard.\n","            (default is False)\n","\n","    Returns:\n","        list: list of fields.\n","    \"\"\"\n","\n","    all_fields = [\n","        'timestamp', \n","        'acc_x_dashboard', 'acc_y_dashboard', 'acc_z_dashboard',\n","        'acc_x_above_suspension', 'acc_y_above_suspension', 'acc_z_above_suspension', \n","        'acc_x_below_suspension', 'acc_y_below_suspension', 'acc_z_below_suspension', \n","        'gyro_x_dashboard', 'gyro_y_dashboard', 'gyro_z_dashboard', \n","        'gyro_x_above_suspension', 'gyro_y_above_suspension', 'gyro_z_above_suspension',\n","        'gyro_x_below_suspension', 'gyro_y_below_suspension', 'gyro_z_below_suspension', \n","        'mag_x_dashboard', 'mag_y_dashboard', 'mag_z_dashboard', \n","        'mag_x_above_suspension', 'mag_y_above_suspension', 'mag_z_above_suspension', \n","        'temp_dashboard', 'temp_above_suspension', 'temp_below_suspension', \n","        'timestamp_gps', 'latitude', 'longitude', 'speed'\n","    ]\n","    \n","    return_fields = []\n","    \n","    for field in all_fields:\n","            \n","        data_type = False\n","        placement = False\n","        \n","        if(speed and field == \"speed\"):\n","            placement = data_type = True\n","            \n","        if(location and (field == \"latitude\" or field == \"longitude\")):\n","            placement = data_type = True\n","        \n","        if(acc):\n","            data_type = data_type or field.startswith(\"acc_\")\n","        \n","        if(gyro):\n","            data_type = data_type or field.startswith(\"gyro_\")\n","            \n","        if(mag):\n","            data_type = data_type or field.startswith(\"mag_\")\n","            \n","        if(temp):\n","            data_type = data_type or field.startswith(\"temp_\")\n","            \n","        if(below_suspension):\n","            placement = placement or field.endswith(\"_below_suspension\")\n","            \n","        if(above_suspension):\n","            placement = placement or field.endswith(\"_above_suspension\")\n","            \n","        if(dashboard):\n","            placement = placement or field.endswith(\"_dashboard\")\n","        \n","        if(data_type and placement):\n","            return_fields.append(field)\n","            \n","    return return_fields\n","\n","def getSubSets(datasets, fields, labels):\n","\n","    \"\"\"Get subsets from raw datasets. For each PVS dataset, extract a subset with only fields/labels passed.\n","\n","    Args:\n","        datasets (dict): raw PVS datasets.\n","        fields (string[]): fields to extract.\n","        labels (string[]): labels to extract.\n","\n","    Returns:\n","        dict: subsets in dict form:\n","        { \n","            \"pvs_x\": { \n","                \"left\": DataFrame, \n","                \"right\": DataFrame, \n","                \"labels\": DataFrame \n","            } \n","        }\n","    \"\"\"\n","\n","    subsets = {}\n","    \n","    for key in datasets.keys():\n","        \n","        subsets[key] = {\n","            \"left\": datasets[key][\"left\"][fields],\n","            \"right\": datasets[key][\"right\"][fields],\n","            \"labels\": datasets[key][\"labels\"][labels]\n","        }\n","    \n","    return subsets\n","\n","def getNormalizedDataMinMax(subsets, scaler_range):\n","\n","    \"\"\"Get normalized data. Use MinMaxScaler.\n","\n","    Args:\n","        subsets (dict): subsets to be normalized.\n","        scaler_range (tuple): range to scale, such as (0,1) or (-1,1).\n","\n","    Returns:\n","        dict: subsets normalized in dict form:\n","        { \n","            \"pvs_x\": { \n","                \"left\": DataFrame, \n","                \"right\": DataFrame, \n","                \"labels\": DataFrame \n","            } \n","        }\n","    \"\"\"\n","\n","    scaler = MinMaxScaler(feature_range=scaler_range)\n","    return getNormalizedData(subsets, scaler)\n","\n","def getNormalizedDataRobust(subsets): \n","\n","    \"\"\"Get standardized data. Use RobustScaler.\n","\n","    Args:\n","        subsets (dict): subsets to be standardized.\n","\n","    Returns:\n","        dict: subsets normalized in dict form:\n","        { \n","            \"pvs_x\": { \n","                \"left\": DataFrame, \n","                \"right\": DataFrame, \n","                \"labels\": DataFrame \n","            } \n","        }\n","    \"\"\"\n","\n","    scaler = RobustScaler()            \n","    return getNormalizedData(subsets, scaler)\n","\n","def getNormalizedData(subsets, scaler):\n","    \n","    \"\"\"Get standardized/normalized data.\n","\n","    Args:\n","        subsets (dict): subsets to be standardized/normalized.\n","        scaler (object): scaler to transform values.\n","\n","    Returns:\n","        dict: subsets normalized in dict form:\n","        { \n","            \"pvs_x\": { \n","                \"left\": DataFrame, \n","                \"right\": DataFrame, \n","                \"labels\": DataFrame \n","            } \n","        }\n","    \"\"\"\n","    \n","    normalized_sets = {}\n","    learn_data = pd.DataFrame()\n","\n","    for pvs in subsets.keys():\n","        for side in [\"left\", \"right\"]:\n","            learn_data = learn_data.append(subsets[pvs][side], ignore_index=True)\n","\n","    scaler = scaler.fit(learn_data)\n","    del learn_data\n","    \n","    for pvs in subsets.keys():\n","        \n","        normalized_sets[pvs] = {\n","            'left':  pd.DataFrame(data=scaler.transform(subsets[pvs]['left']),  columns=subsets[pvs]['left'].columns),\n","            'right': pd.DataFrame(data=scaler.transform(subsets[pvs]['right']), columns=subsets[pvs]['right'].columns),\n","            'labels': subsets[pvs]['labels']\n","        }\n","                    \n","    return normalized_sets # scaler, normalized_sets \n","\n","def getReshapedData(subsets, shape, moving_window, mode_label):  \n","\n","    \"\"\"Reshape data.\n","\n","    Args:\n","        subsets (dict): subsets to be reshaped.\n","        shape (tuple): shape to reshape data. Must have the form (None, ..., ..., features).\n","        moving_window (bool): if used moving window in data reshape.\n","        mode_label (bool): use for output most common value in window, else will be used value at last position in window.\n","\n","    Returns:\n","        dict: subsets reshaped in dict form:\n","        { \n","            \"pvs_x\": { \n","                \"left\": np.array, \n","                \"right\": np.array, \n","                \"labels\": np.array \n","            } \n","        }\n","    \"\"\"\n","\n","    reshaped_sets = {}\n","\n","    shape = tuple([x for x in shape if x is not None])\n","\n","    for key in subsets.keys():\n","\n","        reshaped_sets[key] = {};\n","\n","        for side in ['left', 'right']:\n","\n","            inputs = subsets[key][side].values\n","            outputs = subsets[key][\"labels\"].values\n","  \n","            window = 1\n","\n","            for dim in shape:\n","                window = window * dim\n","\n","            window = int(window / len(inputs[0]))\n","\n","            if moving_window:\n","\n","                inputs_reshaped = []\n","\n","                if mode_label:\n","                    outputs_reshaped = []\n","                else:\n","                    outputs_reshaped = outputs[window-1:]\n","\n","                for i in range(window, len(inputs)+1):\n","                    value = inputs[i-window:i, :]\n","                    value = value.reshape(shape)\n","                    inputs_reshaped.append(value)\n","\n","                    if mode_label:\n","                        outputs_reshaped.append(outputs[i-window:i, :].mean(axis=0).round(0))\n","\n","            else:\n","\n","                inputs_reshaped = []\n","                outputs_reshaped = []\n","\n","                chuncks = int(len(inputs)/window)\n","\n","                for i in range(0, chuncks):\n","                    value = inputs[i*window : (i+1)*window, :]\n","                    value = value.reshape(shape)\n","                    inputs_reshaped.append(value)\n","\n","                    if mode_label:\n","                        outputs_reshaped.append(outputs[i*window : (i+1)*window, :].mean(axis=0).round(0))\n","                    else:\n","                        outputs_reshaped.append(outputs[((i+1)*window)-1])\n","\n","            reshaped_sets[key][side] = np.array(inputs_reshaped) # inputs_reshaped\n","            reshaped_sets[key]['labels'] = np.array(outputs_reshaped) # outputs_reshaped\n","            del inputs_reshaped, outputs_reshaped\n","\n","    return reshaped_sets\n","\n","def getTrainTestSets(preprocessed_sets, sets_train, sets_test, sides=['left', 'right']):\n","\n","    \"\"\"Get train and test sets from pre-processed sets.\n","\n","    Args:\n","        preprocessed_sets (dict): all pre-processed sets.\n","        sets_train (string[]): PVS datasets to be used in train.\n","        sets_test (string[]): PVS datasets to be used in validation.\n","        sides (string[]): PVS datasets to be used. \n","\n","    Returns:\n","        input_train (list|np.array): input train values.\n","        input_test (list|np.array): input validation values. \n","        output_train (list|np.array): output train values.\n","        output_test (list|np.array): output validation values. \n","    \"\"\"\n","\n","    input_train = []\n","    input_test = []\n","    output_train = []\n","    output_test = []\n","\n","    for key in preprocessed_sets.keys():\n","\n","        for side in sides:\n","\n","            inputs = preprocessed_sets[key][side]\n","            outputs = preprocessed_sets[key][\"labels\"]\n","                \n","            if (key in sets_train):\n","\n","                for inp in inputs:\n","                    input_train.append(inp)\n","\n","                for out in outputs:\n","                    output_train.append(out)             \n","              \n","            elif (key in sets_test):\n","\n","                for inp in inputs:\n","                    input_test.append(inp)\n","\n","                for out in outputs:\n","                    output_test.append(out) \n","\n","    return np.array(input_train), np.array(input_test), np.array(output_train), np.array(output_test) # input_train, input_test, output_train, output_test\n","\n","def createMemoryMap(path, input_train, input_test, output_train, output_test):\n","\n","    \"\"\"Create np.array in file memory. Memory map.\n","\n","    Args:\n","        path (string): folder to store maps.\n","        input_train (list|np.array): values for input train.\n","        input_test (list|np.array): values for input validation. \n","        output_train (list|np.array): values for output train.\n","        output_test (list|np.array): values for output validation. \n","\n","    Returns:\n","        it_map (list|np.array): map for input train values.\n","        iv_map (list|np.array): map for input validation values. \n","        ot_map (list|np.array): map for output train values.\n","        ov_map (list|np.array): map for output validation values. \n","    \"\"\"\n","\n","    it_file = os.path.join(path, 'input_train.dat')\n","    it_map = np.memmap(it_file, dtype='float64', mode='w+', shape=((len(input_train),) + input_train[0].shape))\n","    it_map[:] = input_train[:]\n","\n","    iv_file = os.path.join(path, 'input_test.dat')\n","    iv_map = np.memmap(iv_file, dtype='float64', mode='w+', shape=((len(input_test),) + input_test[0].shape))\n","    iv_map[:] = input_test[:]\n","\n","    ot_file = os.path.join(path, 'output_train.dat')\n","    ot_map = np.memmap(ot_file, dtype='float64', mode='w+', shape=((len(output_train),) + output_train[0].shape))\n","    ot_map[:] = output_train[:]\n","\n","    ov_file = os.path.join(path, 'output_test.dat')\n","    ov_map = np.memmap(ov_file, dtype='float64', mode='w+', shape=((len(output_test),) + output_test[0].shape))\n","    ov_map[:] = output_test[:]\n","\n","    return it_map, iv_map, ot_map, ov_map\n","\n","def getFrequencyFeaturesSTFT(subsets, fft_window=fft_window, fft_step=fft_step, fft_mode_label=fft_mode_label):\n","\n","    frequency_sets = {}\n","\n","    for key in subsets.keys():\n","\n","        frequency_sets[key] = {};\n","\n","        for side in ['left', 'right']:\n","\n","            inputs = subsets[key][side]\n","            inputs_new = pd.DataFrame([])\n","\n","            for column in inputs.columns:\n","                frequencies = tf.signal.stft(inputs[column].values, frame_length=fft_window, frame_step=fft_step, fft_length=fft_window)\n","                magnitude = tf.abs(frequencies)\n","                new_np = magnitude.numpy()\n","                new_df = pd.DataFrame(data=new_np, columns=[column + \"_freq_\" + str(x) for x in range(0, new_np.shape[1])])\n","                inputs_new = pd.concat([inputs_new, new_df], axis=1)\n","                del frequencies, magnitude, new_np, new_df\n","\n","            frequency_sets[key][side] = inputs_new\n","            del inputs_new\n","        \n","        outputs = subsets[key]['labels'].values\n","\n","        if fft_mode_label:\n","\n","            outputs_new = []\n","\n","            for i in range(fft_window, len(outputs)+1):\n","                outputs_new.append(outputs[i-fft_window:i, :].mean(axis=0).round(0))\n","\n","        else:\n","            outputs_new = outputs[fft_window-1:]\n","\n","        frequency_sets[key]['labels'] = pd.DataFrame(data=outputs_new, columns=subsets[key]['labels'].columns)\n","        del outputs_new\n","\n","    return frequency_sets"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bB5OQV-5OAlE","colab_type":"code","colab":{}},"source":["def addCompositeAccelerationFeatures(subsets, fields):\n","    \n","    \"\"\"Add composite acceleration features to dataset.\n","    Reference: https://www.sciencedirect.com/science/article/abs/pii/S0966636219306952\n","\n","    Args:\n","        subsets (dict): PVS subsets to add features.\n","        fields (string[]): fields of subsets.\n","\n","    Returns:\n","        subsets (dict): subsets with new features.\n","        fields (string[]): original fields more composite acceleration features. \n","    \"\"\"\n","\n","    acc_x_field = [field for field in fields if \"acc_x\" in field]\n","    acc_y_field = [field for field in fields if \"acc_y\" in field]\n","    acc_z_field = [field for field in fields if \"acc_z\" in field]\n","\n","    fields = [\"acc_x_y\", \"acc_y_z\", \"acc_x_z\", \"acc_x_y_z\"] + fields\n","\n","    for pvs in subsets.keys():\n","        \n","        for side in [\"left\", \"right\"]:\n","        \n","            acc_x_power = np.power(subsets[pvs][side][acc_x_field], 2).values\n","            acc_y_power = np.power(subsets[pvs][side][acc_y_field], 2).values\n","            acc_z_power = np.power(subsets[pvs][side][acc_z_field], 2).values\n","\n","            composite_x_y = np.sqrt(acc_x_power + acc_y_power)\n","            composite_y_z = np.sqrt(acc_y_power + acc_z_power)\n","            composite_x_z = np.sqrt(acc_x_power + acc_z_power)\n","            composite_x_y_z = np.sqrt(acc_x_power + acc_y_power + acc_z_power)\n","\n","            subsets[pvs][side].insert(0, \"acc_x_y\", composite_x_y)\n","            subsets[pvs][side].insert(1, \"acc_y_z\", composite_y_z)\n","            subsets[pvs][side].insert(2, \"acc_x_z\", composite_x_z)\n","            subsets[pvs][side].insert(3, \"acc_x_y_z\", composite_x_y_z)\n","\n","    return subsets, fields"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VvAuMxbDlV5b","colab_type":"text"},"source":["<h5>Model Management</h5>"]},{"cell_type":"code","metadata":{"id":"WU14cNYZlV5c","colab_type":"code","colab":{}},"source":["def createPathIfNotExists(path):\n","\n","    if not os.path.exists(path):\n","        os.makedirs(path)\n","\n","def modelFileSavedFormat(file):\n","    return file + '-train-acc-{acc:.5f}-val-acc-{val_acc:.5f}.hdf5'\n","\n","def saveModelDiagram(model, path, file, show=True):\n","    createPathIfNotExists(path)\n","    plot_model(model, to_file=os.path.join(path, file + '.png'), show_shapes=True, show_layer_names=True)\n","\n","    if show:\n","        display(plot_model(model, show_shapes=True, show_layer_names=True))\n","        display(model.summary())\n","    \n","def showHistory(history):\n","    \n","    for key in history.history.keys():\n","        plt.plot(history.history[key], label=key)\n","    \n","    plt.legend()\n","    \n","def fitModel(model, inputs_train, outputs_train, inputs_validation, outputs_validation, path, file, batch_size=64, epochs=10000, patience=50):\n","    \n","    createPathIfNotExists(path)\n","    \n","    # train_folder = os.path.join(path, 'train')\n","    # createPathIfNotExists(train_folder)\n","    # checkpoint_file_train = os.path.join(train_folder, 'checkpoint-train-{epoch:002d}-{loss:.10f}-{acc:.5f}-val-{val_loss:.10f}-{val_acc:.5f}.hdf5')\n","    # checkpoint_train = ModelCheckpoint(filepath=checkpoint_file_train, save_best_only=True, monitor='acc', mode='max')\n","\n","    # validation_folder = os.path.join(path, 'validation')\n","    # createPathIfNotExists(validation_folder)\n","    # checkpoint_file_validation = os.path.join(validation_folder, 'checkpoint-{epoch:002d}-train-{loss:.10f}-{acc:.5f}-val-{val_loss:.10f}-{val_acc:.5f}.hdf5')\n","    # checkpoint_validation = ModelCheckpoint(filepath=checkpoint_file_validation, save_best_only=True, monitor='val_acc', mode='max', verbose=1)\n","\n","    checkpoint_file_validation = os.path.join(path, modelFileSavedFormat(file))\n","    checkpoint_validation = ModelCheckpoint(filepath=checkpoint_file_validation, save_best_only=True, monitor='val_acc', mode='max') # verbose=1\n","    \n","    # logger_file = os.path.join(path, file + '-training-log.csv')\n","    # csv_logger = CSVLogger(logger_file, append=True)\n","    \n","    plotlosses = PlotLossesKeras()\n","    \n","    early_stopping = EarlyStopping(monitor='val_acc', mode='max', verbose=1, patience=patience, min_delta=0.0001, restore_best_weights=True)\n","    \n","    # callbacks=[csv_logger, checkpoint_train, checkpoint_validation, early_stopping]\n","    callbacks=[plotlosses, checkpoint_validation, early_stopping]\n","    return model.fit(inputs_train, outputs_train, validation_data=(inputs_validation, outputs_validation), epochs=epochs, batch_size=batch_size, callbacks=callbacks, verbose=0) # verbose=1\n","\n","def predictModel(model, inputs):\n","    return model.predict(inputs)\n","    \n","def evaluateModel(model, inputs, outputs, batch_size=64):\n","    return model.evaluate(inputs, outputs, batch_size=batch_size, verbose=0)\n","    \n","def loadWeights(model, pathFile):\n","    model.load_weights(pathFile)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ymM2zyFSlV5g","colab_type":"text"},"source":["<h5>Parameter Variations</h5>"]},{"cell_type":"code","metadata":{"id":"o8TDC5VdlV5i","colab_type":"code","colab":{}},"source":["experiment_by_dataset = [\n","    { \"train\": [\"pvs_1\", \"pvs_3\", \"pvs_4\", \"pvs_6\", \"pvs_7\", \"pvs_9\"], \"test\":  [\"pvs_2\", \"pvs_5\", \"pvs_8\"]},\n","    { \"train\": [\"pvs_1\", \"pvs_2\", \"pvs_3\", \"pvs_7\", \"pvs_8\", \"pvs_9\"], \"test\":  [\"pvs_4\", \"pvs_5\", \"pvs_6\"]},\n","    { \"train\": [\"pvs_1\", \"pvs_2\", \"pvs_4\", \"pvs_6\", \"pvs_8\", \"pvs_9\"], \"test\":  [\"pvs_3\", \"pvs_5\", \"pvs_7\"]}\n","]\n","\n","experiment_by_fields = [\n","    (\"Below Suspension\", getFields(acc=True, gyro=True, speed=True, below_suspension=True)),\n","    (\"Above Suspension\", getFields(acc=True, gyro=True, speed=True, above_suspension=True)),\n","    (\"Dashboard\",        getFields(acc=True, gyro=True, speed=True, dashboard=True))\n","]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"E42CTyM2lV5l","colab_type":"text"},"source":["<h5>Labels Fields</h5>"]},{"cell_type":"code","metadata":{"id":"W9TUEwohlV5m","colab_type":"code","colab":{}},"source":["surface_type_labels = [\"land\", \"cobblestone\", \"asphalt\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4i3UwyMpGGqo","colab_type":"code","colab":{}},"source":["surface_type_labels_plot = [\"Dirt \\n Road\", \"Cobblestone \\n Road\", \"Asphalt \\n Road\"]  "],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yfwQdDlHxkZ_","colab_type":"text"},"source":["<h5>Execution Log</h5>"]},{"cell_type":"code","metadata":{"id":"hIw1Rq3Z9hnC","colab_type":"code","colab":{}},"source":["# Save a log for each experiment execution (params for each execution)\n","def saveExecutionLog(path, data, columns=['placement', 'experiment', \"window\", \"scaler\", \"input_shape\", \"output_shape\", \"train_loss\", \"val_loss\", \"train_acc\", \"val_acc\"]):\n","    save = pd.DataFrame(data=data, columns=columns)\n","    save.to_csv(os.path.join(path, \"experiment-execution-log.csv\"), index=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y2vDUTUC3LbT","colab_type":"text"},"source":["<h5>Training Functions</h5>"]},{"cell_type":"code","metadata":{"id":"OZ_h99QuxnuW","colab_type":"code","colab":{}},"source":["def manageFiles(history, experiment_folder, experiment_file):\n","\n","    test = -1\n","    index = -1\n","    val_acc = -1\n","    \n","    for i in range(0,3):\n","\n","        max_value = max(history[i]['val_acc'])\n","\n","        if max_value > val_acc:\n","            val_acc = max_value\n","            test = i\n","            index = history[i]['val_acc'].index(max_value)\n","            \n","    train_acc = history[test]['acc'][index]\n","    train_loss = history[test]['loss'][index]\n","    val_acc = history[test]['val_acc'][index]\n","    val_loss = history[test]['val_loss'][index]\n","    \n","    test_folder = os.path.join(experiment_folder, \"Test \" + str(test + 1)) \n","    file = modelFileSavedFormat(experiment_file).format(**{'acc': train_acc, 'val_acc': val_acc})\n","\n","    move_from = os.path.join(test_folder, file)\n","    move_to = os.path.join(experiment_folder, file)\n","\n","    shutil.move(move_from, move_to)\n","    \n","    for i in range(0,3):\n","        shutil.rmtree(os.path.join(experiment_folder, \"Test \" + str(i + 1)))\n","\n","    return [train_loss, val_loss, train_acc, val_acc]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4ppZlIZI3eLz","colab_type":"code","colab":{}},"source":["def getLoadBar():\n","    \n","    global load_bar_placement, load_bar_dataset, load_bar_experiment, load_bar_retries\n","    \n","    experiment_total_placement = len(experiment_by_fields)\n","    experiment_total_dataset = len(experiment_by_dataset)\n","    experiment_total_iteration = len(input_shapes)\n","    \n","    load_bar_placement = tqdm(total=experiment_total_placement, desc='Placement Progress')\n","    load_bar_dataset = tqdm(total=experiment_total_dataset, desc='Dataset Progress')\n","    load_bar_experiment = tqdm(total=experiment_total_iteration, desc='Input Shapes Progress')\n","    load_bar_retries = tqdm(total=3, desc='Retries')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gJS-OGcs4R9O","colab_type":"code","colab":{}},"source":["def run(model_fn, batch_size=64, patience=50, epochs=10000, retries=3, addCompositeAccFeatures=False, frequencyDomain=False):\n","\n","    load_bar_placement.reset()\n","\n","    for placement, fields in experiment_by_fields:\n","\n","        load_bar_placement.set_description(placement)\n","        load_bar_dataset.reset()\n","        \n","        subsets = getSubSets(datasets.copy(), fields, surface_type_labels)\n","\n","        if addCompositeAccFeatures:\n","            subsets, fields = addCompositeAccelerationFeatures(subsets, fields)\n","\n","        if frequencyDomain:\n","            subsets = getFrequencyFeaturesSTFT(subsets)\n","            # fields = []\n","            normalized_sets = getNormalizedDataMinMax(subsets, (0,1))\n","        else:\n","            normalized_sets = getNormalizedDataMinMax(subsets, (-1,1))\n","        \n","        del subsets\n","        gc.collect()\n","\n","        for experiment_number in range(0, len(experiment_by_dataset)):\n","\n","            load_bar_experiment.reset()\n","            load_bar_experiment.set_description(\"Experiment \" + str(experiment_number + 1))\n","\n","            sets_train = experiment_by_dataset[experiment_number]['train']\n","            sets_test = experiment_by_dataset[experiment_number]['test']\n","\n","            execution_log = []\n","\n","            for input_shape, window_size in input_shapes:\n","\n","                model_args = parameters(input_shape, output_shape)\n","\n","                history = []\n","\n","                reshaped_sets = getReshapedData(normalized_sets.copy(), input_shape, moving_window, mode_label)\n","                input_train, input_test, output_train, output_test = getTrainTestSets(reshaped_sets, sets_train, sets_test, sides)\n","                del reshaped_sets\n","                gc.collect()\n","\n","                print(\"Input Train Shape:\", input_train.shape, \"Output Train Shape:\", output_train.shape)\n","                print(\"Input Validation Shape:\", input_test.shape, \"Output Validation Shape:\", output_test.shape)\n","\n","                for test in range(0, retries):\n","\n","                    model, model_name = model_fn(**model_args)\n","\n","                    experiment_folder = os.path.join(experiments_folder, model_name, placement, \"Experiment \" + str(experiment_number + 1))\n","                    test_folder = os.path.join(experiment_folder, \"Test \" + str(test + 1))\n","                    diagram_file = \"experiment-\" + str(experiment_number + 1) + \"-window-\" + str(window_size)\n","                    experiment_file = diagram_file + \"-min-max-scaler-\" + (\"(0,1)\" if frequencyDomain else \"(-1,1)\")\n","\n","                    saveModelDiagram(model, experiment_folder, diagram_file)\n","                    hist = fitModel(model, input_train, output_train, input_test, output_test, test_folder, experiment_file, patience=patience, batch_size=batch_size, epochs=epochs)\n","\n","                    history.append(hist.history)\n","                    load_bar_retries.update(1)\n","\n","                    # Clean Memory\n","                    del model, hist\n","                    # model, hist = [None, None]\n","                    \n","                    if not(output is None):\n","                        output.clear()\n","\n","                    gc.collect()\n","\n","                metrics = manageFiles(history, experiment_folder, experiment_file)\n","                \n","                execution_log.append([\n","                    placement,\n","                    experiment_number + 1, \n","                    window_size, \n","                    \"Min Max Scaler\" + (\"(0,1)\" if frequencyDomain else \"(-1,1)\"), \n","                    str(input_shape), \n","                    str(output_shape)\n","                ] + metrics)\n","\n","                saveExecutionLog(experiment_folder, execution_log)\n","                load_bar_experiment.update(1)\n","\n","                # Clean\n","                del history, input_train, input_test, output_train, output_test, metrics\n","                load_bar_retries.reset()\n","                gc.collect()\n","                time.sleep(5)\n","\n","            load_bar_dataset.update(1)       \n","        \n","        del normalized_sets\n","        load_bar_placement.update(1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Boeyib9csLVj","colab_type":"text"},"source":["<h5>Plot Functions</h5>"]},{"cell_type":"code","metadata":{"id":"-d9MBAlczYj2","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1593307142279,"user_tz":180,"elapsed":722,"user":{"displayName":"Jeferson Menegazzo","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhQawyEgOlmBHPaOoOT_loAbYF9XYcZK5PiYARmfg=s64","userId":"12080988616245410525"}}},"source":["# Plot best model CNN 7 features window 300 time domain\n","def confusionMatrix(files, title, model_fn, input_shape, output_shape, experiment_by_fields):\n","\n","    placement = experiment_by_fields[0] \n","    fields = experiment_by_fields[1] \n","\n","    matrix = []\n","    subsets = getSubSets(datasets.copy(), fields, surface_type_labels)\n","    normalized_sets = getNormalizedDataMinMax(subsets, (-1,1))\n","    reshaped_sets = getReshapedData(normalized_sets, input_shape, moving_window, mode_label)\n","\n","    for experiment_number in range(0,3):\n","\n","        sets_train = experiment_by_dataset[experiment_number]['train']\n","        sets_test = experiment_by_dataset[experiment_number]['test']\n","\n","        input_train, input_test, output_train, output_test = getTrainTestSets(reshaped_sets.copy(), sets_train, sets_test, sides)\n","\n","        model_args = parameters(input_shape, output_shape)\n","        model, model_name = model_fn(**model_args)\n","        loadWeights(model, os.path.join(experiments_folder, model_name, placement, \"Experiment \" + str(experiment_number + 1), files[experiment_number]))\n","        predictions = predictModel(model, input_test)\n","        matrix.append(confusion_matrix(output_test.argmax(axis=1), predictions.argmax(axis=1), normalize=\"true\"))\n","\n","    values = ((matrix[0] + matrix[1] + matrix[2])/3)*100\n","    con_mat_df = pd.DataFrame(values, index=surface_type_labels_plot, columns=surface_type_labels_plot)\n","    figure = plt.figure(figsize=(4,4))\n","    sns.set(font_scale=1.2)\n","    sns.heatmap(con_mat_df, annot=True, cmap=plt.cm.Blues, annot_kws={\"size\": 14})\n","    plt.tight_layout()\n","    plt.title(title)\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    plt.show()\n","    figure.savefig('confusion_matrix.png', bbox_inches=\"tight\")"],"execution_count":1,"outputs":[]},{"cell_type":"code","metadata":{"id":"xWcb8bptHvPb","colab_type":"code","colab":{}},"source":["# https://towardsdatascience.com/how-to-easily-process-audio-on-your-gpu-with-tensorflow-2d9d91360f06\n","def plotMagnitudeSpectogram(freq, sampling_rate, fft_step):\n","    S = freq.T\n","    plt.figure(figsize=(20,6))\n","    lr.display.specshow(S, y_axis='linear', x_axis='frames', sr=sampling_rate, hop_length=fft_step)\n","    plt.colorbar()\n","    plt.title('Magnitude Spectrogram')\n","    plt.tight_layout()\n","    plt.ylabel('Frequency (Hz)')\n","    plt.xlabel('Time (10ms)')\n","    plt.savefig('magnitude_spectogram_librosa.png', dpi=300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rKwqMuYyeqt1","colab_type":"code","colab":{}},"source":["# https://towardsdatascience.com/how-to-easily-process-audio-on-your-gpu-with-tensorflow-2d9d91360f06\n","def plotLogMagnitudeSpectogram(freq, sampling_rate, fft_step):\n","    S = freq.T\n","    plt.figure(figsize=(20,6))\n","    lr.display.specshow(lr.amplitude_to_db(S, ref=np.max), y_axis='linear', x_axis='frames', sr=sampling_rate, hop_length=fft_step)\n","    plt.colorbar(format='%+2.0f Db')\n","    plt.title('Log-Magnitude Spectrogram')\n","    plt.tight_layout()\n","    plt.ylabel('Frequency (Hz)')\n","    plt.xlabel('Time (10ms)')\n","    plt.savefig('log_magnitude_spectogram_librosa.png', dpi=300)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tA9oq4fUfnpl","colab_type":"code","colab":{}},"source":["print(\"V2\")"],"execution_count":null,"outputs":[]}]}